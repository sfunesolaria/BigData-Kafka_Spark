{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Sergio Funes Olaria\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7999b5da34520ecf6bb63d1eda26bfd5",
     "grade": false,
     "grade_id": "logos",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)  ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c82856d1ced38213fc3860b2f9d6ecbc",
     "grade": false,
     "grade_id": "intro_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# PEC_5_2: Structured Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "123e7730f23a4a19be4280a802c891b8",
     "grade": false,
     "grade_id": "intro_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "En esta PEC vamos a trabajar con [Spark Structured Streaming](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html), un motor de procesamiento de flujo escalable y tolerante a fallos construido sobre el motor Spark SQL. \n",
    "\n",
    "Spark Structured Streaming nos permite realizar nuestro análisis de datos en streaming de la misma manera que lo hacemos con el procesamiento por lotes sobre datos estáticos. Ahora bien, hay que tener en cuenta que el structured streaming tiene una serie de ventajas. Por ejemplo, que el motor Spark SQL se encargará de ejecutar los analísis programados de forma incremental y continua, generando el resultado final a medida que datos de transmisión. Spark Streaming se basa en la API de Dataset/DataFrame que se puede utilizar Scala, Java, Python o R para expresar agregaciones de transmisión, ventanas de tiempo de eventos, etc. Finalmente, el sistema asegura garantías de tolerancia a fallos de un extremo a otro a través de puntos de control y registros de escritura anticipada.\n",
    "\n",
    "\n",
    "**IMPORTANTE: Para realizar esta práctica debes hacerlo mediante SSH desde terminal o VSCODE, y poner el código de la misma en este NOTEBOOK solo para su corrección.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0edfcd16d7cc2be22efd3561ff651ef",
     "grade": false,
     "grade_id": "entrega",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### entrega: \n",
    "El formato de entrega será un directorio comprimido en formato gnuzip bajo el nombre `PEC5_username.tar.gz`, substituyendo username por vuestro\tnombre\tde\tusuario. El contenido debe ser un fichero para cada programa Python por cada ejercicio indicando el apartado de los notebooks de enunciado, por ejemplo `PEC5_username_2_1_4.py` .Adjuntar los dos notebooks de la PEC, con el nombre `PEC5_1_username`, y `PEC5_2_username` con las salidas obtenidas que se piden, y las respuestas a las preguntas conceptuales planteadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87ffdb73f2d251d84399ec3c19e1abd2",
     "grade": false,
     "grade_id": "intro_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. PARTE 1. Word Count con Structured Streaming\n",
    "1. PARTE 2. Operaciones de ventana sobre eventos temporales\n",
    "1. PARTE 3. Captura y procesamiento de datos en tiempo real de la API OpenSky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "531165577e5611b2e2f75fa2041d63e9",
     "grade": false,
     "grade_id": "intro_1_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## PARTE 1. Word Count con Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ada6b9be4e906fd0e2247dd27cd76c20",
     "grade": false,
     "grade_id": "intro_1_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "En esta primera parte de la PEC vamos a ver como implementar un word count conectando por sockets mediante un proceso netcat https://en.wikipedia.org/wiki/Netcat corriendo en una terminal vía SSH o VSCode y donde vais a ir escribiendo palabras, que posteriormente van a ser contadas.\n",
    "\n",
    "Para empezar, vamos a realizar un primer ejercicio guiado donde vamos a contar las palabras haciendo uso de los DataFrames que nos ofrece Structured Streaming. \n",
    "\n",
    "La siguiente celda de Jupyter Notebook crea un objeto spark que corresponde a una instancia de SparkSession. En las versiones modernas de Spark, la clase SparkSession es el punto de entrada a una aplicación Spark para cualquier tipo de Spark API (RDD, SparkSQL, Streaming, etc). Se pide ejecutar la siguiente celda y comprobar que se ha ejecutado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcd7af8becd4d1ded76f3a136c354125",
     "grade": false,
     "grade_id": "intro_1_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "# Introducid el nombre de la app PEC5_ seguido de vuestro nombre de usuario\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_sgraul\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0ece4b688f5dc795f8672c708863bbb",
     "grade": false,
     "grade_id": "intro_1_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Mediante el objeto spark vamos a configurar una lectura de datos en streaming reportados en el puerto que tenéis asociado, dado que es donde está el netcat estará funcionando. En el código de la siguiente celda debéis cambiar \\<PUERTO_ASIGNADO\\> por vuestro puerto.\n",
    "\n",
    "El DataFrame `linesDF` representa una tabla ilimitada que contiene la transmisión de datos de texto. Esta tabla contiene una columna de cadenas denominada `value`, y cada línea de los datos de texto de transmisión se convierte en una fila de la tabla. Tened en cuenta que todavía no está recibiendo ningún dato ya que solo estamos configurando la transformación y aún no hemos comenzado a recibir datos. Se pide al estudiante leer el código con detalle, revisar que se entienden todas las operaciones (consultar documentación en caso necesario) y ejecutar la celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c20d15f5fc553ea44d16cd7260753bb0",
     "grade": false,
     "grade_id": "intro_1_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Creamos el DataFrame representando el streaming de las lineas que nos entran por host:port\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', 'localhost')\\\n",
    "    .option('port', <PUERTO_ASIGNADO>)\\\n",
    "    .load()\n",
    "\n",
    "# Separamos las lineas en palabras en un nuevo DF\n",
    "#las funciones explode y split estan explicadas en\n",
    "#https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html\n",
    "wordsDF = linesDF.select(\n",
    "    explode(\n",
    "        split(linesDF.value, ' ')\n",
    "    ).alias('palabra')\n",
    ")\n",
    "\n",
    "# Generamos el word count en tiempo de ejecución\n",
    "wordCountsDF = wordsDF.groupBy('palabra').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e7c05dd69b313e06b5de126d2ad9691",
     "grade": false,
     "grade_id": "intro_1_6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora que hemos configurado la consulta (análisis) sobre los datos de transmisión, declaramos la consulta para comenzar a recibir los datos y contar las palabras. Para hacer esto, vamos a configurar la salida del análisis para que imprima el conjunto completo de recuentos, especificado por `outputMode(\"complete\")` y configurado para trabajar en memoria cada vez que se actualizan. Finalmente iniciamos el cálculo de streaming usando `start()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df2f00811f41f1f183c003c14167cf1b",
     "grade": false,
     "grade_id": "intro_1_7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Iniciamos la consuta que muestra por consola o almacena en memoria el word count. \n",
    "# Trabajamos a partir del DataFrame que contiene la agrupación de las palabras y el numero de repeticiones\n",
    "# Utilizamos el formato memory para poder mostrarlo en Notebook, \n",
    "#si ejecutamos en consola debemos poner el formato console\n",
    "query = wordCountsDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode('complete')\\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"palabras\") \\\n",
    "    .start()\n",
    "\n",
    "#en una ejecución desde el terminal de sistema, necesitamos evitar que el programa finalice mientras \n",
    "#se está ejecutando la consulta en un Thread separado y en segundo plano. \n",
    "#query.awaitTermination() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ab82a1c6e259e56bcc9a5127a17c139",
     "grade": false,
     "grade_id": "intro_1_8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "En una **sesión de terminal mediante SSH, no mediante Jupyter terminal** debéis ejecutar un netcat `$ nc -lk <puerto_asignado>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29e169e6f6e19680038080db5fb3c6d7",
     "grade": false,
     "grade_id": "intro_1_10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Mediante esta celda podemos mostrar en el Notebook los datos de la consulta a la tabla `palabras` en una celda, y vamos actualizando esta celda cada 5 segundos. En este caso utilizamos una sentencia SQL. Como se trata de un bucle sobre el Notebook deberéis parar el kernel una vez vista la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89e3b3e2150f43ece760da72511cd267",
     "grade": false,
     "grade_id": "intro_1_11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(spark.sql('SELECT * FROM palabras').show())\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e1b0ea10c287fbb0da25369ffafdbe1",
     "grade": false,
     "grade_id": "out_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "`{'isDataAvailable': False,\n",
    " 'isTriggerActive': True,\n",
    " 'message': 'Waiting for data to arrive'}\n",
    "+-------+-----+\n",
    "|palabra|count|\n",
    "+-------+-----+\n",
    "|   Data|    2|\n",
    "|    UOC|    2|\n",
    "|    Big|    2|\n",
    "|  Spark|    1|\n",
    "+-------+-----+`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "245ef74107308b5b0f13e50336ffc1e8",
     "grade": false,
     "grade_id": "intro_1_8_b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Alternativamente podemos consultar los datos que estamos recibiendo por streaming mediante la tabla `palabras`, pero tendremos que actualizar manualmente el show(). Tarda un tiempo en aparecer la primera salida en Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e11fe2a1d5254717127c43d3df009008",
     "grade": false,
     "grade_id": "intro_1_9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"palabras\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e6f9eabdab9e25b12f90421cb0896ee",
     "grade": false,
     "grade_id": "out_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "`+-------+-----+\n",
    "|palabra|count|\n",
    "+-------+-----+\n",
    "|   Data|    2|\n",
    "|    UOC|    2|\n",
    "|    Big|    2|\n",
    "|  Spark|    1|\n",
    "+-------+-----+`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "51a6f675e66fec697c4bcc8539f04e5d",
     "grade": false,
     "grade_id": "cell-43d7b0107f924383",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A partir de este ejemplo que hemos visto y que el alumno debe ejecutar para probar su funcionamiento, se pide:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "415d9d6637dfc10900afa9801f260f7f",
     "grade": false,
     "grade_id": "task_1_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Pregunta 1. (1 punto)** Realiza un programa en Python que cuente las palabras que empiezan por A y que tengan más de 5 counts. Debéis ejecutarlo el programa en una **terminal**, no dentro del Jupyter, y en otra terminal el netcat. Para ello utilizamos un programa Python en local mediante `./python3 PEC5_2_1_1.py localhost <puerto_asignado>`  \n",
    "\n",
    "> Adjunta el código y la salida obtenida en **forma textual**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b83f758c0a481d9c888ee65ec2f2e7a2",
     "grade": false,
     "grade_id": "out_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "`+-------+-----+\n",
    "|palabra|count|\n",
    "+-------+-----+\n",
    "| Albert|    6|\n",
    "+-------+-----+`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24ec0f240694795c3124870a0bc9b857",
     "grade": true,
     "grade_id": "answer_1_1_a",
     "locked": false,
     "points": 0.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_sfunesolaria\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', 'localhost')\\\n",
    "    .option('port', 20068)\\\n",
    "    .load()\n",
    "\n",
    "wordsDF = linesDF.select(\n",
    "    explode(\n",
    "        split(linesDF.value, ' ')\n",
    "    ).alias('palabra')\n",
    ")\n",
    "\n",
    "wordCountsDF = wordsDF.groupBy('palabra').count()\n",
    "\n",
    "query = wordCountsDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode('complete')\\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"palabras\") \\\n",
    "    .start()\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(spark.sql(\"SELECT * FROM palabras WHERE palabra LIKE 'A%' AND count > 5\").show())\n",
    "    sleep(5)\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71e62a24732355a28f1035d3968eebc1",
     "grade": false,
     "grade_id": "cell-d4ecb863c0062b6a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copia la salida obtenida en formato de texto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b61bd1ca809dfb57abf18170d66d1b49",
     "grade": true,
     "grade_id": "answer_1_1_b",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "{'message': 'Waiting for data to arrive', 'isTriggerActive': False, 'isDataAvailable': False}\n",
    "+-------+-----+\n",
    "|palabra|count|\n",
    "+-------+-----+\n",
    "|  Adios|    6|\n",
    "+-------+-----+\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a8f72ff828d9c6d0e06f999aee33a21",
     "grade": false,
     "grade_id": "task_1_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora vamos a realizar un ejercicio que nos permita realizar una consulta SQL sobre los datos recibidos.  Además, utilizaremos el mecanismo de control de fallos que Spark utiliza, los [*checkpoint*](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing), que van guardando información en el HDFS por si es necesario recuperarla. \n",
    "\n",
    ">**Pregunta 2. (1 punto)** Crea una tabla temporal para poder realizar una consulta SQL sobre las palabras que estamos obteniendo mediante streaming. El programa debe extraer las diferentes palabras de una frase y solo mostrar por consola aquellas que tengan una longitud superior a 3 caracteres. Tenéis que mostrar el tiempo de adquisición y poner un checkpoint en HDFS que se denomine `punto_control_pec5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2630185e0d18cabdc8ce0a38024304f6",
     "grade": false,
     "grade_id": "out_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "`\n",
    "+-------+--------------------+\n",
    "|palabra|              tiempo|\n",
    "+-------+--------------------+\n",
    "|   Data|2021-12-2  12:21:...|\n",
    "+-------+--------------------+\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e7cef53060e9d76daf780a37828d8b",
     "grade": true,
     "grade_id": "answer_1_2_a",
     "locked": false,
     "points": 0.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_sfunesolaria\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', 'localhost')\\\n",
    "    .option('port', 20068)\\\n",
    "    .load()\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "wordsDF = linesDF.select(\n",
    "    explode(\n",
    "        split(linesDF.value, ' ')\n",
    "    ).alias('palabra'), current_timestamp().alias('tiempo')\n",
    ")\n",
    "\n",
    "wordCountsDF = wordsDF.groupBy('palabra', 'tiempo').count()\n",
    "\n",
    "query = wordCountsDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode('complete')\\\n",
    "    .option(\"checkpointLocation\", \"/user/sfunesolaria/PEC5/punto_control_pec5\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"palabras\") \\\n",
    "    .start()\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(spark.sql('SELECT palabra, tiempo FROM palabras WHERE LENGTH(palabra) > 3').show())\n",
    "    sleep(5)\n",
    "\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e260e5fc8f578fd0640158bb713f34fa",
     "grade": false,
     "grade_id": "cell-874ffab8d3c2535e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copia la salida obtenida en formato de texto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d850a84b41af627e43f354b5974fcad",
     "grade": true,
     "grade_id": "answer_1_2_b",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "{'message': 'Waiting for data to arrive', 'isTriggerActive': False, 'isDataAvailable': False}\n",
    "+-------+--------------------+\n",
    "|palabra|              tiempo|\n",
    "+-------+--------------------+\n",
    "|   Data|2022-01-17 12:27:...|\n",
    "+-------+--------------------+\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e751fd7c4fdba163f88d6f489ac78457",
     "grade": false,
     "grade_id": "task_1_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 3. (1 punto)** Modifica el programa para que haga uso del outputMode *append*. Debemos de guardar cada entrada en un fichero de texto en HDFS. Adjunta la salida del HDFS del contenido del directorio creado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71a3c0050f7856c05977c0b34dd6ba8b",
     "grade": false,
     "grade_id": "out_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "`\n",
    "hdfs dfs -ls /user/<usuario>/pec5_1_3\n",
    "Found 4 items\n",
    "drwxr-xr-x   - usuario usuario          0 2021-12-02 12:43 /user/<usuario>/pec5_1_3/_spark_metadata\n",
    "-rw-r--r--   3 usuario usuario          9 2021-12-02 12:43 /user/<usuario>/pec5_1_3/part-00000-499014ff-cf00-4f2f-a8a4-d282cdac1a19-c000.txt\n",
    "-rw-r--r--   3 usuario usuario          7 2021-12-02 12:43 /user/<usuario>/pec5_1_3/part-00000-7d8cb984-95ad-4e50-8887-a72f4d2814a2-c000.txt\n",
    "-rw-r--r--   3 usuario usuario          0 2021-12-02 12:43 /user/<usuario>/pec5_1_3/part-00000-a4918ce0-c930-439f-a5cd-a1bd777f609b-c000.txt\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "949ba4e58a1502950b1a315768eff26a",
     "grade": true,
     "grade_id": "answer_1_3_a",
     "locked": false,
     "points": 0.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_sfunesolaria\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', 'localhost')\\\n",
    "    .option('port', 20068)\\\n",
    "    .load()\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "wordsDF = linesDF.select(\n",
    "    explode(\n",
    "        split(linesDF.value, ' ')\n",
    "    ).alias('palabra'), current_timestamp().alias('tiempo')\n",
    ")\n",
    "\n",
    "wordCountsDF = wordsDF.withWatermark(\"tiempo\", \"1 second\").groupBy('palabra', 'tiempo').count()\n",
    "\n",
    "query = wordCountsDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode('append')\\\n",
    "    .option(\"checkpointLocation\", \"/user/sfunesolaria/pec5_1_3\") \\\n",
    "    .option(\"hdfs_url\", \"hdfs:///user/sfunesolaria/pec5_1_3\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"palabras\") \\\n",
    "    .start()\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(spark.sql('SELECT palabra, tiempo FROM palabras WHERE LENGTH(palabra) > 3').show())\n",
    "    sleep(5)\n",
    "\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17e76b1ae84cb6b3930194d0d1428016",
     "grade": false,
     "grade_id": "cell-4ea5b9edeae46258",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copia la salida obtenida en el HDFS en formato de texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68b2c35d48a8baeec7439a1c135616c9",
     "grade": true,
     "grade_id": "answer_1_3_b",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8cbde4a8ca1e01e2a478b4bff3b1ff74",
     "grade": false,
     "grade_id": "task_1_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 4.(1 punto)** Realiza un programa en Python para que haga uso del outputMode update, y que los datos entrantes por consola. La lectura debe realizar en intervalos de 5 segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d6597dd5ca80757219b1f530ad883df",
     "grade": false,
     "grade_id": "out_6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "<code>\n",
    "-------------------------------------------\n",
    "Batch: 5\n",
    "-------------------------------------------\n",
    "+-------+\n",
    "|palabra|\n",
    "+-------+\n",
    "|    Big|\n",
    "|   Data|\n",
    "| Hadoop|\n",
    "+-------+\n",
    "<5 ... segundos>\n",
    "-------------------------------------------\n",
    "Batch: 6\n",
    "-------------------------------------------\n",
    "+-------+\n",
    "|palabra|\n",
    "+-------+\n",
    "|  Spark|\n",
    "+-------+\n",
    "    </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6aebc7fbc4c5063ca03a82cf3eb8754",
     "grade": true,
     "grade_id": "answer_1_4_a",
     "locked": false,
     "points": 0.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a151c7925f831e68b21582124d49707",
     "grade": false,
     "grade_id": "cell-f981a227281aa19e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copia la salida obtenida en formato de texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d61e4bbff433e54388bf72817ddb9ff3",
     "grade": true,
     "grade_id": "answer_1_4_b",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45eb40cf552c9e4da2678d4fd1e2815a",
     "grade": false,
     "grade_id": "task_1_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 5.(1 punto)** Explica las diferencias y similitudes entre los tipos de salidas existentes en Structured Streaming (complete, update y append). El texto debe ser claro, explicativo y tener una extensión de 10 líneas aproximadamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee8eef197e3fb242072795189cebaeff",
     "grade": true,
     "grade_id": "answer_1_5",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "784f0c45d66c91f2eda7de23db4814e2",
     "grade": false,
     "grade_id": "intro_2_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## PARTE 2. Operaciones de ventana sobre eventos temporales\n",
    "En esta parte vamos a trabajar con operaciones de ventana sobre eventos temporales. Para ello vamos a utilizar el formato *rate*. El source [RateStreamSource](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#api-using-datasets-and-dataframes) es una fuente de transmisión que genera números consecutivos con marca de tiempo y es utilizada habitualmente para hacer pruebas y PoC (*Proof of Concept*). Para configurar un RateStreamSource utilizaremos  `format('rate')`, y el esquema de los datos entrantes es el adjunto siguiente. A diferencia de los ejercicios de la parte 1 no tendremos dos programas corriendo simultáneamente en el terminal, solo tendremos una, la de nuestro programa pyspark, dado que el formato rate se controla directamente desde la configuración del source Spark.\n",
    "\n",
    "\n",
    "`root\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- value: long (nullable = true) `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "feb4bcc33ff3a306bc9f4614eb2e493f",
     "grade": false,
     "grade_id": "task_2_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 1. (1 punto)** Realiza un programa mediante Structured Streaming que genere números mediante un formato *rate* como origen del streaming y donde debéis realizar el tratamiento de los mismos para acumularlos. Los números deben generarse cada segundo, y debemos utilizar una ventana de agrupación del streaming de 10 segundos y que se actualice cada 5 segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85721b3e66a790221f30a43543326db4",
     "grade": true,
     "grade_id": "answer_2_1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_sfunesolaria\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('rate')\\\n",
    "    .option('rowsPerSecond', '1')\\\n",
    "    .load()\n",
    "\n",
    "wordsDF = linesDF.withColumn(\"window\", linesDF.timestamp)\\\n",
    "                .withColumn(\"value\", linesDF.value)\n",
    "\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "wordCountsDF = wordsDF.groupBy(window('window', \"10 seconds\", \"5 seconds\"), 'value').count()\n",
    "\n",
    "query = wordCountsDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode('complete')\\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"palabras\") \\\n",
    "    .start()\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(spark.sql('SELECT * FROM palabras').show())\n",
    "    sleep(5)\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0bd3551f6a2a2734a224516689efaf6",
     "grade": false,
     "grade_id": "task_2_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 2. (1 punto)** Comenta el código, muestra la salida que has obtenido y coméntala en una extensión en 4 y 8 líneas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75c6bca1f50a1eaa52db1b45a797699e",
     "grade": false,
     "grade_id": "out_7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "<code>\n",
    "-------------------------------------------                                     \n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+------------------------------------------+-----+-----+\n",
    "|window                                    |value|count|\n",
    "+------------------------------------------+-----+-----+\n",
    "|[2021-12-03 10:33:40, 2021-12-03 10:33:50]|0    |1    |\n",
    "|[2021-12-03 10:33:45, 2021-12-03 10:33:55]|5    |1    |\n",
    "|[2021-12-03 10:33:45, 2021-12-03 10:33:55]|0    |1    |\n",
    "|[2021-12-03 10:33:45, 2021-12-03 10:33:55]|3    |1    |\n",
    "|[2021-12-03 10:33:45, 2021-12-03 10:33:55]|4    |1    |\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cecd0636f84a9495da34157226c550d",
     "grade": true,
     "grade_id": "answer_2_2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "Se ha utilizado un código similar al explicado en el primer ejercicio. A la variable spark se le añade el formato rate para configurar un RateStreamSource y la opción rowsPerSecond para indicar que los numeros se generen cada segundo. A la variable linesDF se le indica que la columna window sea un timestamp y value contenga el numero generado. La variable wordsDF se agrupa por la variable window y value y se cuentan en la columna count. El resto del codigo es exactamente igual al del primer ejercicio.\n",
    "\n",
    "{'message': 'Processing new data', 'isTriggerActive': True, 'isDataAvailable': True}\n",
    "+--------------------+-----+-----+                                              \n",
    "|              window|value|count|\n",
    "+--------------------+-----+-----+\n",
    "|[2022-01-18 03:31...|    8|    1|\n",
    "|[2022-01-18 03:31...|   24|    1|\n",
    "|[2022-01-18 03:31...|   25|    1|\n",
    "|[2022-01-18 03:31...|    6|    1|\n",
    "|[2022-01-18 03:31...|    1|    1|\n",
    "|[2022-01-18 03:31...|   10|    1|\n",
    "|[2022-01-18 03:31...|   22|    1|\n",
    "|[2022-01-18 03:31...|   11|    1|\n",
    "|[2022-01-18 03:31...|    8|    1|\n",
    "|[2022-01-18 03:31...|   18|    1|\n",
    "|[2022-01-18 03:31...|   17|    1|\n",
    "|[2022-01-18 03:31...|   24|    1|\n",
    "|[2022-01-18 03:31...|    7|    1|\n",
    "|[2022-01-18 03:31...|    7|    1|\n",
    "|[2022-01-18 03:31...|   15|    1|\n",
    "|[2022-01-18 03:31...|   21|    1|\n",
    "|[2022-01-18 03:31...|    9|    1|\n",
    "|[2022-01-18 03:31...|    3|    1|\n",
    "|[2022-01-18 03:31...|   15|    1|\n",
    "|[2022-01-18 03:31...|   23|    1|\n",
    "+--------------------+-----+-----+\n",
    "only showing top 20 rows\n",
    "\n",
    "En la salida puede verse en cada fila de la columna window el timestamp de cuando se genera y cuando se recibe, con una diferencia de 5 segundos entre los dos timestamp. En la columna value se encuentra el valor generado y la columna count contiene el contado de los valores según el valor de la columna window y value. El valor de count siempre será 1 si no coinciden el timestamp y el número. En el ejemplo de salida, no pueden verse todos los valores porque la tabla es muy ancha.\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3f816157b1003d0b6cff5e88c3dfd7a",
     "grade": false,
     "grade_id": "task_2_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "En la ejecución de las consultas es muy interesante poder ir obteniendo información sobre el progreso realizado en el último disparador del flujo: qué datos se procesaron, cuáles fueron las tasas de procesamiento, latencias, etc.\n",
    "\n",
    ">**Pregunta 3 (1 punto).** Modifica el programa para que muestre 3 [métricas](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#monitoring-streaming-queries) del streaming mientras este se realiza. Solo se deben mostrar métricas mientras la consulta está activa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b76c64b787e16d2d540a1383742c8a8",
     "grade": true,
     "grade_id": "answer_2_3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_sfunesolaria\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('rate')\\\n",
    "    .option('rowsPerSecond', '1')\\\n",
    "    .load()\n",
    "\n",
    "wordsDF = linesDF.withColumn(\"window\", linesDF.timestamp)\\\n",
    "                .withColumn(\"value\", linesDF.value)\n",
    "\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "wordCountsDF = wordsDF.groupBy(window('window', \"10 seconds\", \"5 seconds\"), 'value').count()\n",
    "\n",
    "query = wordCountsDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode('complete')\\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"palabras\") \\\n",
    "    .start()\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(query.lastProgress)\n",
    "    display(query.recentProgress)\n",
    "    sleep(5)\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57da1fc6fca61d2cec1dcb840787e03a",
     "grade": false,
     "grade_id": "task_2_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 4 (1 punto).** Explica las 3 métricas aplicadas con una extensión entre 5 y 10 líneas propias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff223ee18dd86791e02181cbb66402fc",
     "grade": false,
     "grade_id": "out_8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "<code>\n",
    "{'isDataAvailable': False, 'isTriggerActive': False, 'message': 'Initializing sources'}\n",
    "{'stateOperators': [{'customMetrics': {'loadedMapCacheHitCount': 0, 'stateOnCurrentVersionSizeBytes': 25198, 'loadedMapCacheMissCount': 0}, 'numRowsUpdated': 0, 'memoryUsedBytes': 82798, 'numRowsTotal': 0}], 'timestamp': '2021-12-03T10:16:01.657Z', 'sources': [{'description': 'RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default', 'endOffset': 0, 'startOffset': None, 'processedRowsPerSecond': 0.0, 'numInputRows': 0}], 'runId': '9134994e-c17f-4277-974a-21cd09cf9aea', 'durationMs': {'triggerExecution': 36450, 'walCommit': 48, 'getB[Stage 8:======>        (22 + 2) / 200]\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68677f26c2ac1f5b13b4695f87a6fbdb",
     "grade": true,
     "grade_id": "answer_2_4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "Salida de ejemplo:\n",
    "\n",
    "{'isTriggerActive': True, 'message': 'Processing new data', 'isDataAvailable': True}\n",
    "{'batchId': 0, 'sources': [{'numInputRows': 0, 'processedRowsPerSecond': 0.0, 'endOffset': 0, 'description': 'RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default', 'startOffset': None}], 'processedRowsPerSecond': 0.0, 'numInputRows': 0, 'stateOperators': [{'numRowsUpdated': 0, 'customMetrics': {'stateOnCurrentVersionSizeBytes': 12599, 'loadedMapCacheHitCount': 0, 'loadedMapCacheMissCount': 0}, 'memoryUsedBytes': 41399, 'numRowsTotal': 0}], 'id': '86598058-bb77-471c-8f24-7a7e5dbc6138', 'runId': 'db0a1e30-5e1f-4a2a-bb24-4005ee7496de', 'durationMs': {'setOffsetRange': 1, 'getBatch': 7, 'addBatch': 25323, 'getEndOffset': 0, 'triggerExecution': 25906, 'walCommit': 148, 'queryPlanning': 323}, 'sink': {'description': 'MemorySink'}, 'name': 'palabras', 'timestamp': '2022-01-18T03:06:35.539Z'}\n",
    "[{'batchId': 0, 'sources': [{'numInputRows': 0, 'processedRowsPerSecond': 0.0, 'endOffset': 0, 'description': 'RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default', 'startOffset': None}], 'processedRowsPerSecond': 0.0, 'numInputRows': 0, 'stateOperators': [{'numRowsUpdated': 0, 'customMetrics': {'stateOnCurrentVersionSizeBytes': 12599, 'loadedMapCacheHitCount': 0, 'loadedMapCacheMissCount': 0}, 'memoryUsedBytes': 41399, 'numRowsTotal': 0}], 'id': '86598058-bb77-471c-8f24-7a7e5dbc6138', 'runId': 'db0a1e30-5e1f-4a2a-bb24-4005ee7496de', 'durationMs': {'setOffsetRange': 1, 'getBatch': 7, 'addBatch': 25323, 'getEndOffset': 0, 'triggerExecution': 25906, 'walCommit': 148, 'queryPlanning': 323}, 'sink': {'description': 'MemorySink'}, 'name': 'palabras', 'timestamp': '2022-01-18T03:06:35.539Z'}]\n",
    "\n",
    "Utilizando lastProgress devuelve un objeto StreamingQueryProgress en Scalay Java y un diccionario en Python. Tiene toda la información sobre el progreso realizado en el último disparo del flujo (qué datos fueron procesados, cuáles fueron las tasas de procesamiento, latencias, etc.). Usando recentProgress devuelve un array de los últimos progresos. Utilizando status da información sobre lo que la consulta está haciendo inmediatamente (es un disparador activo, los datos que están siendo procesados, etc.)\n",
    "\n",
    "Fuente: https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#monitoring-streaming-queries\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ca4dccb18ba42b31ab1988999714669",
     "grade": false,
     "grade_id": "intro_3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## PARTE 3. Captura y procesamiento de datos en tiempo real con la API OpenSky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6821ac85e43507dabf5a8ff513cf8822",
     "grade": false,
     "grade_id": "intro_3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Es esta parte de la práctica vamos a trabajar la adquisición de datos en tiempo real de [OpenSky](https://opensky-network.org/). OpenSky Network es una asociación sin ánimo de lucro con sede en Suiza que brinda acceso abierto a los datos de control de seguimiento de vuelos.  Fue creado como un proyecto de investigación por varias universidades y entidades gubernamentales con el objetivo de mejorar la seguridad, confiabilidad y eficiencia del espacio aéreo. Su función principal es recopilar, procesar y almacenar datos de control de tráfico aéreo y proporcionar acceso abierto a estos datos al público. Esencialmente los datos de los aviones se obtienen vía satélite haciendo uso de  Automatic Dependent Surveillance–Broadcast (ADS–B). Para realizar este ejercicio no es necesario registrarse en el sistema OpenSky dado que vamos ha relizar actualizaciones de la información e vuelo sobre la superficie de España cada 10 segundos. La API está disponible este [enlace](https://openskynetwork.github.io/opensky-api/python.html). El parámetro bbox es una tupla que indica la latitud mínima, máxima, y las longitudes mínimas y máximas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f204eac8ff253369dd44eb65beafd35",
     "grade": false,
     "grade_id": "intro_3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Primeramente, vamos a utilizar el servicio OpenSkyApi para leer un rectángulo con las latitudes y longitudes que engloban la península ibérica.\n",
    "\n",
    "Para ello debéis [instalar](https://github.com/openskynetwork/opensky-api) la biblioteca en vuestro directorio del servidor Cloudera\n",
    "\n",
    "1. Descargar en formato .zip el repositorio\n",
    "1. Subir a vuestro directorio personal del servidor de Cloudera el zip. \n",
    "1. Descomprimirlo.\n",
    "1. Dentro del directorio que ha creador ejecutar `pip install -e ./python`\n",
    "\n",
    "Una vez instalada el módulo anterior, la siguiente celda os mostrará los vuelos registrados sobre la península ibérica en estos momentos. Observad con detenimiento las propiedades del diccionario de cada vuelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba1422e695cfac097b08aaabe243cdcf",
     "grade": false,
     "grade_id": "into_3_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from random import sample\n",
    "\n",
    "from opensky_api import OpenSkyApi\n",
    "api = OpenSkyApi()\n",
    "states = api.get_states(bbox=(36.173357, 44.024422,-10.137019, 1.736138))\n",
    "#recuperamos codigo, pais_origen, long, lat, altitud, velocidad, ratio_vertical\n",
    "#atención en este ejemplo solo estamos mostrando 5 vuelos aleatorios, \n",
    "#en vuestros ejercicios deberéis eliminar la función sample\n",
    "for s in sample(states.states,5):\n",
    "    vuelo_dict = {\n",
    "                'callsign':s.callsign,\n",
    "                'country': s.origin_country,\n",
    "                'longitude': s.longitude,\n",
    "                'latitude': s.latitude,\n",
    "                'velocity': s.velocity,\n",
    "                'vertical_rate': s.vertical_rate,\n",
    "            }\n",
    "    vuelo_encode_data = json.dumps(vuelo_dict, indent=2).encode('utf-8')\n",
    "    print(\"(%r, %r,%r, %r, %r, %r)\" % (s.callsign, s.origin_country, s.longitude, s.latitude,s.velocity,s.vertical_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4cf6b46d367d23ac0646e61dc4437e4",
     "grade": false,
     "grade_id": "out_10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "`('BAW457  ', 'United Kingdom',-3.5196, 40.4292, 86.45, 10.73)\n",
    "('BLX245  ', 'Sweden',-6.0307, 43.8266, 252.51, 0)\n",
    "('CFG1HE  ', 'Germany',-8.4689, 40.2967, 236.56, 0)\n",
    "('TOM3MK  ', 'United Kingdom',-7.2687, 41.5878, 247.02, 0)\n",
    "('AEA57MC ', 'Spain',-0.5364, 38.2791, 64.7, -3.9)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30a4aef1194952c5cccc756fffa5d04b",
     "grade": false,
     "grade_id": "into_3_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora vamos a crear un programa en Python para poder enviar cada 10 segundos por el puerto que tenéis asignado información de los vuelos que hay sobre la península ibérica. Deberéis poner en marcha primero el programa Python con el servidor de sockets que lee de Opensky y después el programa de Spark con structured streaming, es decir, en esta parte volvemos a tener dos terminales abiertas a la vez, y lo podéis realizar con el VSCode o con el SSH."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5df565ecbd6b9daf81cb3d2934a363a",
     "grade": false,
     "grade_id": "task_3_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 1. (1 punto)** Modifica el programa Python para enviar datos de los vuelos en formato JSON. Os podéis auxiliar de la función [json.dumps](https://docs.python.org/3/library/json.html) que nos permite crear un JSON binario de cada diccionario con las propiedades del vuelo. Prestad atención al salto de línea, '\\n', que se adjunta al final de cada envío, es fundamental para cerrar la transmisión de datos a Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d2da5d413e7fd00bc872d823c08180f",
     "grade": true,
     "grade_id": "answer_3_1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "from time import sleep\n",
    "import socket\n",
    "import json\n",
    "from opensky_api import OpenSkyApi\n",
    "\n",
    "HOST = 'localhost'  # hostname o IP address\n",
    "PORT = 20068         # puerto socket server\n",
    "\n",
    "api = OpenSkyApi()\n",
    "states = api.get_states(bbox=(36.173357, 44.024422,-10.137019, 1.736138))\n",
    "\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "s.bind((HOST, PORT))\n",
    "s.listen(1)\n",
    "while True:\n",
    "    print('\\nEscoltant per un client a',HOST , PORT)\n",
    "    conn, addr = s.accept()\n",
    "    print('\\Connectat per', addr)\n",
    "    try:\n",
    "        while(True):\n",
    "            v = {}\n",
    "            for vuelo in states.states:\n",
    "                v = {\n",
    "                    'callsign':vuelo.callsign,\n",
    "                    'country': vuelo.origin_country,\n",
    "                    'longitude': vuelo.longitude,\n",
    "                    'latitude': vuelo.latitude,\n",
    "                    'velocity': vuelo.velocity,\n",
    "                    'vertical_rate': vuelo.vertical_rate,\n",
    "                }\n",
    "                print(v)\n",
    "                conn.send(json.dumps(v).encode('utf-8'))\n",
    "                conn.send(b'\\n')\n",
    "            sleep(10)       \n",
    "    except socket.error:\n",
    "        print ('Error .\\n\\nClient desconnectat.\\n')\n",
    "conn.close()\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca28e20d52c477f90cd98b43a87b31a9",
     "grade": false,
     "grade_id": "task_3_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 2. (1 punto)** Se pide leer los datos recibidos mediante structured streaming y mostrar el esquema de los datos recibidos. En este primer ejercicio solo vamos a tener una cadena con el JSON recibido de cada vuelo y un esquema con un único elemento. Debéis utilizar la función \"printSchema()\".\n",
    "\n",
    ">Una vez comprobada que la transmisión funciona, se pide realizar un pre-procesado antes del envío de los datos mediante el socket para eliminar aquellas líneas de datos que no sean útiles ni convenientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b20b6823016a47894ea3d4c91cd802a4",
     "grade": false,
     "grade_id": "out_11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "<code>\n",
    "root\n",
    " |-- value: string (nullable = true)\n",
    "\n",
    "|value             |\n",
    "\n",
    "|{\"velocity\": 210.12, \"vertical_rate\": 0, \"latitude\": 43.6082, \"callsign\": \"TAP441  \", \"longitude\": -1.3992, \"country\": \"Portugal\"}         |\n",
    "|{\"velocity\": 246.5, \"vertical_rate\": 0, \"latitude\": 40.5836, \"callsign\": \"TAP844  \", \"longitude\": -3.8452, \"country\": \"Portugal\"}          |\n",
    "|{\"velocity\": 0, \"vertical_rate\": null, \"latitude\": 40.487, \"callsign\": \"IBE2800 \", \"longitude\": -3.5889, \"country\": \"Spain\"}      \n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03d6aa1be4af17718b153ab9f1670237",
     "grade": true,
     "grade_id": "answer_3_2_1",
     "locked": false,
     "points": 0.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_sfunesolaria\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', 'localhost')\\\n",
    "    .option('port', 20068)\\\n",
    "    .load()\n",
    "\n",
    "wordsDF = linesDF.withColumn(\"value\", linesDF.value)\n",
    "\n",
    "wordCountsDF = wordsDF.groupBy('value').count()\n",
    "\n",
    "query = wordCountsDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode('complete')\\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"palabras\") \\\n",
    "    .start()\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(wordsDF.printSchema())\n",
    "    display(spark.sql('SELECT value FROM palabras').show())\n",
    "    sleep(5)\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5456f5a970048c1ac355f71fa0b7d150",
     "grade": false,
     "grade_id": "cell-cf210fa5a9e69efd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copia la salida obtenida en formato de texto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcb6ebd68d329632f046ee871a8a44be",
     "grade": true,
     "grade_id": "answer_3_2_2",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "root\n",
    " |-- value: string (nullable = true)\n",
    "\n",
    "None\n",
    "+--------------------+                                                          \n",
    "|               value|\n",
    "+--------------------+\n",
    "|{\"country\": \"Spai...|\n",
    "|{\"country\": \"Irel...|\n",
    "|{\"country\": \"Aust...|\n",
    "|{\"country\": \"Germ...|\n",
    "|{\"country\": \"Spai...|\n",
    "|{\"country\": \"Spai...|\n",
    "|{\"country\": \"Port...|\n",
    "|{\"country\": \"Spai...|\n",
    "|{\"country\": \"Spai...|\n",
    "|{\"country\": \"Swit...|\n",
    "|{\"country\": \"Spai...|\n",
    "|{\"country\": \"Spai...|\n",
    "|{\"country\": \"Spai...|\n",
    "|{\"country\": \"Spai...|\n",
    "|{\"country\": \"Swed...|\n",
    "|{\"country\": \"Germ...|\n",
    "|{\"country\": \"Spai...|\n",
    "|{\"country\": \"King...|\n",
    "|{\"country\": \"Spai...|\n",
    "|{\"country\": \"Port...|\n",
    "+--------------------+\n",
    "only showing top 20 rows\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1265dd033225e67a6512f4f3d6e917fa",
     "grade": false,
     "grade_id": "task_3_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 3. (1 punto)** De pide mostrar la información en forma de tabla, con las columnas, country|callsign|longitude|latitude|velocity|vertical_rate. Para ello vais a tener que crear un esquema mediante [StructType](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html) y aplicarlo a la función SQL [from_json](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.from_json.html). De esta manera podremos pasar de una columna string con todo el JSON, a 6 columnas con el tipo ajustado al valor contenido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "375fbcfcd617d3b84c0ad94883628627",
     "grade": false,
     "grade_id": "out_12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "<code>\n",
    "root\n",
    " |-- callsign: string (nullable = true)\n",
    " |-- country: string (nullable = true)\n",
    " |-- longitude: long (nullable = true)\n",
    " |-- latitude: long (nullable = true)\n",
    " |-- velocity: long (nullable = true)\n",
    " |-- vertical_rate: long (nullable = true)\n",
    "\n",
    "\n",
    "+--------------+--------+---------+--------+--------+-------------+\n",
    "|       country|callsign|longitude|latitude|velocity|vertical_rate|\n",
    "+--------------+--------+---------+--------+--------+-------------+\n",
    "|      Portugal|TAP441  |  -1.3992| 43.6082|  210.12|          0.0|\n",
    "|      Portugal|TAP844  |  -3.8452| 40.5836|   246.5|          0.0|\n",
    "|         Spain|IBE2800 |  -3.5889|  40.487|     0.0|         null|\n",
    "|United Kingdom|ABW713  |  -0.5287|  41.899|  155.82|        -7.48|\n",
    "|         Spain|FYS161  |   -0.188| 38.8394|   64.77|        -2.93|\n",
    "|         Spain|IBE3242 |  -3.5896| 40.4919|    0.77|         null|\n",
    "|         Spain|AEA4025 |   0.1208| 38.5346|  125.53|         3.25|\n",
    "|         Spain|P21     |  -3.5728| 40.4751|   10.29|         null|\n",
    "|         Spain|IBE30EA |  -2.6086|  40.914|  181.29|        -6.18|\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17662377ee9afa8dc6e11941824a72f6",
     "grade": true,
     "grade_id": "answer_3_3_1",
     "locked": false,
     "points": 0.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_sfunesolaria\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', 'localhost')\\\n",
    "    .option('port', 20068)\\\n",
    "    .load()\n",
    "\n",
    "wordsDF = linesDF.withColumn(\"value\", linesDF.value)\n",
    "\n",
    "wordCountsDF = wordsDF.groupBy('value').count()\n",
    "\n",
    "from pyspark.sql.types import StringType, DoubleType, StructType, StructField\n",
    "from pyspark.sql.functions import from_json, col\n",
    "jsonSchema = StructType([ StructField(\"callsign\", StringType(), True),\n",
    "                          StructField(\"velocity\", DoubleType(), True),\n",
    "                          StructField(\"longitude\", DoubleType(), True),\n",
    "                          StructField(\"latitude\", DoubleType(), True),\n",
    "                          StructField(\"country\", StringType(), True),\n",
    "                          StructField(\"vertical_rate\", DoubleType(), True)\n",
    "                        ])\n",
    "\n",
    "df = wordCountsDF.withColumn(\"value\", from_json(col(\"value\"), jsonSchema))\n",
    "df_select = df.select(\"value.callsign\", \"value.country\", \"value.longitude\", \"value.latitude\", \"value.velocity\",\n",
    "                      \"value.vertical_rate\")\n",
    "\n",
    "query = df_select\\\n",
    "    .writeStream\\\n",
    "    .outputMode('complete')\\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"palabras\") \\\n",
    "    .start()\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(df_select.printSchema())\n",
    "    display(spark.sql('SELECT country, callsign, longitude, latitude, velocity, vertical_rate FROM palabras').show())\n",
    "    sleep(5)\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c06cd85ac27df93beefb9fe3f123c39",
     "grade": false,
     "grade_id": "cell-07353f460967c893",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copia la salida obtenida en formato de texto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "366df90301f27f080a894c1808edf125",
     "grade": true,
     "grade_id": "answer_3_3_2",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "root\n",
    " |-- callsign: string (nullable = true)\n",
    " |-- country: string (nullable = true)\n",
    " |-- longitude: double (nullable = true)\n",
    " |-- latitude: double (nullable = true)\n",
    " |-- velocity: double (nullable = true)\n",
    " |-- vertical_rate: double (nullable = true)\n",
    "\n",
    "None\n",
    "+--------------------+--------+---------+--------+--------+-------------+       \n",
    "|             country|callsign|longitude|latitude|velocity|vertical_rate|\n",
    "+--------------------+--------+---------+--------+--------+-------------+\n",
    "|               Spain|VLG73WB |  -5.3908| 38.7958|  234.17|        -0.33|\n",
    "|             Ireland|RYR2NT  |  -2.6438| 37.8678|  228.89|          0.0|\n",
    "|             Austria|EJU7674 |  -5.7166| 39.6921|  211.93|        -0.33|\n",
    "|             Germany|OCN300  |  -2.3122| 40.5742|  239.12|          0.0|\n",
    "|               Spain|VLG78LM |    0.162|  39.954|  231.12|          3.9|\n",
    "|               Spain|ANE18LY |  -3.3822| 39.5037|   234.2|         6.18|\n",
    "|            Portugal|TAP846F |  -2.5077| 40.9977|  229.33|          0.0|\n",
    "|               Spain|SWT7773 |   1.5802| 38.9577|   106.1|        -3.25|\n",
    "|               Spain|AEA056  |  -6.1163| 41.2638|  243.94|        -0.33|\n",
    "|               Spain|IBS39UC |  -6.0401| 37.5052|  112.74|         -2.6|\n",
    "|         Switzerland|CAZ601  |   -2.805| 38.0225|   247.8|        -13.0|\n",
    "|               Spain|IBS3946 |  -8.1522| 38.1785|  225.48|         0.33|\n",
    "|               Spain|IBE05MV |  -8.3019| 42.4179|  139.09|        -7.48|\n",
    "|               Spain|IBE3490 |   1.5487| 43.7652|  227.59|          0.0|\n",
    "|              Sweden|NOZ5070 |  -1.4336| 43.9072|  230.64|          0.0|\n",
    "|             Germany|GAF906  |  -0.4259| 42.7904|  247.98|          0.0|\n",
    "|               Spain|FAUNA2  |  -3.5442| 40.4783|    6.43|         null|\n",
    "|Kingdom of the Ne...|TFL605  |  -7.5212| 40.7335|  235.57|          0.0|\n",
    "|               Spain|AEA33TV |  -2.6347| 40.9556|  180.86|         -5.2|\n",
    "|            Portugal|TAP088  |  -9.1433| 38.7677|    68.5|        -3.25|\n",
    "+--------------------+--------+---------+--------+--------+-------------+\n",
    "only showing top 20 rows\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f1cb2896ad74d3c09bfa59b17677916",
     "grade": false,
     "grade_id": "task_3_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 4. (1 punto)** Muestra el total de vuelos para cada destino agrupados por país de destino que hay en cada momento. Los datos deben mostrarse ordenados por país alfabéticamente. Tened en cuenta que podemos recibir datos duplicados dado que el script de OpenSky lee cada 10 segundos todos los vuelos existentes y los envía por socket. Por defecto Spark crea 200 tareas (cada una implicará una partición de los datos) por stage en el procesamiento en Structured Streaming. Para acelerar el proceso de captura, se pide que [ajustéis](https://spark.apache.org/docs/latest/sql-performance-tuning.html#other-configuration-options) el parámetro en la configuración de SparkSession a un valor de 4 particiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc257a5fbb2e9db8ca67f5490044aef8",
     "grade": false,
     "grade_id": "out_13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "<code>\n",
    "-------------------------------------------\n",
    "Batch: 4\n",
    "-------------------------------------------\n",
    "+--------------------------+-----+\n",
    "|country                   |count|\n",
    "+--------------------------+-----+\n",
    "|Algeria                   |1    |\n",
    "|Austria                   |3    |\n",
    "|Belgium                   |1    |\n",
    "|Chile                     |2    |\n",
    "|Denmark                   |1    |\n",
    "|France                    |15   |\n",
    "|Germany                   |15   |\n",
    "|Hungary                   |1    |\n",
    "|Ireland                   |26   |\n",
    "|Kingdom of the Netherlands|2    |\n",
    "|Lithuania                 |1    |\n",
    "|Luxembourg                |1    |\n",
    "|Malta                     |5    |\n",
    "|Mexico                    |1    |\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fda41a55d99c7f0c38e05b804812cef",
     "grade": true,
     "grade_id": "answer_3_4_1",
     "locked": false,
     "points": 0.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_sfunesolaria\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', 'localhost')\\\n",
    "    .option('port', 20068)\\\n",
    "    .load()\n",
    "\n",
    "wordsDF = linesDF.withColumn(\"value\", linesDF.value)\n",
    "\n",
    "from pyspark.sql.types import StringType, DoubleType, StructType, StructField\n",
    "from pyspark.sql.functions import from_json, col, asc\n",
    "jsonSchema = StructType([ StructField(\"callsign\", StringType(), True),\n",
    "                          StructField(\"velocity\", DoubleType(), True),\n",
    "                          StructField(\"longitude\", DoubleType(), True),\n",
    "                          StructField(\"latitude\", DoubleType(), True),\n",
    "                          StructField(\"country\", StringType(), True),\n",
    "                          StructField(\"vertical_rate\", DoubleType(), True)\n",
    "                        ])\n",
    "\n",
    "df = wordsDF.withColumn(\"value\", from_json(col(\"value\"), jsonSchema))\n",
    "df_select = df.select(\"value.country\").groupBy('country').count()\n",
    "\n",
    "query = df_select\\\n",
    "    .writeStream\\\n",
    "    .outputMode('complete')\\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"palabras\") \\\n",
    "    .start()\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(spark.sql('SELECT country, count FROM palabras').orderBy(asc(\"country\")).show())\n",
    "    sleep(5)\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95d206d775b46cf03921a4ad5ae7eb29",
     "grade": false,
     "grade_id": "cell-a2a12feeee643a28",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copia la salida obtenida en formato de texto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b848a83be1b3c5d91e85976c88fc1d7c",
     "grade": true,
     "grade_id": "answer_3_4_1_2",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "None\n",
    "+--------------------+-----+                                                    \n",
    "|             country|count|\n",
    "+--------------------+-----+\n",
    "|             Austria|    6|\n",
    "|             Belgium|    6|\n",
    "|              Brazil|    6|\n",
    "|               Chile|    6|\n",
    "|      Czech Republic|    6|\n",
    "|             Denmark|    9|\n",
    "|               Egypt|    3|\n",
    "|             Finland|    3|\n",
    "|              France|   21|\n",
    "|             Germany|   30|\n",
    "|             Hungary|    3|\n",
    "|             Ireland|   87|\n",
    "|               Italy|    3|\n",
    "|Kingdom of the Ne...|    6|\n",
    "|               Malta|   12|\n",
    "|              Mexico|    3|\n",
    "|             Morocco|    3|\n",
    "|              Poland|    9|\n",
    "|            Portugal|   48|\n",
    "|               Qatar|    6|\n",
    "+--------------------+-----+\n",
    "only showing top 20 rows\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa7479591bce24fb7c0d521cd32b7325",
     "grade": false,
     "grade_id": "task_3_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 5. (1 punto)** Agrupa todos los vuelos que están subiendo en altura, los que están bajando y los que están en tierra. Indica su numero. Deberás auxiliarte de una consulta SQL para poder indicar con -1 que un vuelo está descendiendo, +1 si está subiendo, y 0 si está en tierra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65db97c981a4344ce7cf80027f6220d1",
     "grade": false,
     "grade_id": "out_14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "<code>\n",
    "+------+-----+\n",
    "|estado|count|\n",
    "+------+-----+\n",
    "|     0|   96|\n",
    "|    -1|   54|\n",
    "|     1|   41|\n",
    "+------+-----+\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8aca0ff370a46419939efbdb1aa6c80",
     "grade": true,
     "grade_id": "answer_3_5_1",
     "locked": false,
     "points": 1.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_sfunesolaria\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', 'localhost')\\\n",
    "    .option('port', 20068)\\\n",
    "    .load()\n",
    "\n",
    "wordsDF = linesDF.withColumn(\"value\", linesDF.value)\n",
    "\n",
    "from pyspark.sql.types import StringType, DoubleType, StructType, StructField\n",
    "from pyspark.sql.functions import from_json, col, when\n",
    "jsonSchema = StructType([ StructField(\"callsign\", StringType(), True),\n",
    "                          StructField(\"velocity\", DoubleType(), True),\n",
    "                          StructField(\"longitude\", DoubleType(), True),\n",
    "                          StructField(\"latitude\", DoubleType(), True),\n",
    "                          StructField(\"country\", StringType(), True),\n",
    "                          StructField(\"vertical_rate\", DoubleType(), True)\n",
    "                        ])\n",
    "\n",
    "df = wordsDF.withColumn(\"value\", from_json(col(\"value\"), jsonSchema))\n",
    "df_select = df.select(\"value.vertical_rate\").filter(col(\"vertical_rate\").isNotNull()) \\\n",
    "            .withColumn(\"estado\", when((col(\"vertical_rate\") < 0), -1).when((col(\"vertical_rate\") > 0), 1).otherwise(0) ) \\\n",
    "            .groupBy('estado').count()\n",
    "\n",
    "query = df_select\\\n",
    "    .writeStream\\\n",
    "    .outputMode('complete')\\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"palabras\") \\\n",
    "    .start()\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(spark.sql('SELECT * FROM palabras').show())\n",
    "    sleep(5)\n",
    "\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49c7cbd77f16864bdcd875c08bf48f6e",
     "grade": false,
     "grade_id": "cell-a528e031ad020c15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copia la salida obtenida en formato de texto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9fe7de196b10377bceb1a57d5245769",
     "grade": true,
     "grade_id": "answer_3_5_2",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "None\n",
    "+------+-----+                                                                  \n",
    "|estado|count|\n",
    "+------+-----+\n",
    "|    -1|  189|\n",
    "|     1|  144|\n",
    "|     0|  201|\n",
    "+------+-----+\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d681d971225d4b5238e30c1968fd6bdf",
     "grade": false,
     "grade_id": "task_3_6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 6. (1 punto)** ¿De que manera podemos identificar que aparece un nuevo vuelo en el espacio aéreo?. No hace falta escribir el código sino describir con palabras como se plantearía la solución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a0ab2e972f7fae3ffec91c08e983043",
     "grade": true,
     "grade_id": "answer_3_6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "Identificaremos los vuelos por el valor de callsign que identifica la señal del avión. Si el vuelo tiene valor de callsign significa que la señal se recibe y si callsign es None, quiere decir que no se recibe la señal.\n",
    "\n",
    "Si utilizamos la información de la tabla de la pregunta 3, tendremos todos los valores que se reciben. Si agrupamos los valores por callsign, obtendremos todos los vuelos actuales. Si obtenemos un valor nuevo de callsign en la llegada de datos, quiere decir que es un nuevo vuelo.\n",
    "\n",
    "Por otro lado, si usamos la API de OpenSky, la solución más sencilla es comparar los valores de la variable on_ground y comprobar cuando el valor pasa a TRUE, querrá decir que el avión no está en tierra.\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "352a02192f44fb4c5b21b0018f632fe5",
     "grade": false,
     "grade_id": "task_3_7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 7. (1 punto)** Explica brevemente en una extensión de entre 5 y 10 lineas las ventajas e inconvenientes de utilizar Structured Streaming versus Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "683dbd7c5774c93b576f5bbc396dc641",
     "grade": true,
     "grade_id": "answer_3_7",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "Si hablamos de la transmisión real de datos, Spark Streaming funciona en lotes y los datos se envían después de cada duración del lote, en cambio, en Structured Streaming los datos fluyen continuamente, por tanto, se decanta más por la transmisión real.\n",
    "Si hablamos de rendimiento, Spark Streaming utiliza internamente RDD y Structured Streaming utiliza Dataframe para realizar operaciones de transmisión. Los DataFrames (Structured Streaming) están más optimizados y brindan más opciones de agregación.\n",
    "Si hablamos de la latencia que hay entre la generación de datos y en la entrega de los datos, Spark Streaming coloca los datos en un lote en función de la marca de tiempo lo que puede generar pérdida de datos. Por otro lado, Structured Streaming tiene la funcionalidad de procesar los datos cuando la marca de tiempo se incluye en los datos recibidos.\n",
    "Si hablamos de restricciones y flexibilidad de eso, en Spark Streaming no hay restricción de uso de sink, ya que tiene el método foreachRDD, pero en Structures Streaming hay un número limitado de sink, porque no tiene un método del estilo.\n",
    "Se puede concluir que Structured Streaming es una mejor plataforma de Streaming en comparación con Spark Streaming.\n",
    "\n",
    "Fuente: https://blog.knoldus.com/spark-streaming-vs-structured-streaming/\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
